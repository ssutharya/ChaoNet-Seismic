{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install obspy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "collapsed": true,
        "id": "qx9lvcWSuMm8",
        "outputId": "8c638ed4-b689-4dc6-bff6-04d0cceb6382"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting obspy\n",
            "  Downloading obspy-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from obspy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from obspy) (1.14.1)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from obspy) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from obspy) (5.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from obspy) (75.1.0)\n",
            "Collecting sqlalchemy<2 (from obspy)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from obspy) (2.32.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy) (2.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2->obspy) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->obspy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->obspy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->obspy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->obspy) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.17.0)\n",
            "Downloading obspy-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlalchemy, obspy\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.39\n",
            "    Uninstalling SQLAlchemy-2.0.39:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.39\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed obspy-1.4.1 sqlalchemy-1.4.54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "signal"
                ]
              },
              "id": "f4e60c415e9846739641d343b54f72c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IRIS Dataset - Paper Implementation:"
      ],
      "metadata": {
        "id": "Tf1ETXcjDJjb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFYejOK0LT_g",
        "outputId": "f37775b1-7f14-46bb-c448-c58da65d1871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training TT features:\n",
            " [[0.9        0.68333333 0.91694915 0.9       ]\n",
            " [0.87777778 0.51666667 0.98474576 0.94166667]\n",
            " [0.96111111 0.64166667 0.99830508 0.94166667]\n",
            " [0.96111111 0.68333333 0.96779661 0.98333333]\n",
            " [0.87777778 0.475      0.96779661 0.98333333]\n",
            " [0.71111111 0.85       0.67627119 0.725     ]\n",
            " [0.73888889 0.68333333 0.57457627 0.6       ]\n",
            " [0.37777778 0.64166667 0.43898305 0.51666667]\n",
            " [0.71111111 0.725      0.55762712 0.6       ]\n",
            " [0.71111111 0.68333333 0.55762712 0.64166667]\n",
            " [0.68333333 0.80833333 0.40508475 0.35      ]\n",
            " [0.65555556 0.68333333 0.40508475 0.39166667]\n",
            " [0.18333333 0.68333333 0.15084746 0.26666667]\n",
            " [0.40555556 0.6        0.26949153 0.18333333]\n",
            " [0.29444444 0.68333333 0.28644068 0.475     ]]\n",
            "Mean TT vectors:\n",
            " [array([0.91555556, 0.6       , 0.96711864, 0.95      ]), array([0.65      , 0.71666667, 0.56101695, 0.61666667]), array([0.44444444, 0.69166667, 0.30338983, 0.33333333])]\n",
            "GLS+TT Accuracy (5 samples/class): 94.81%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Modified GLS map\n",
        "def gls_map(x, b, scale=0.7):\n",
        "    if 0 <= x < b:\n",
        "        return x\n",
        "    return scale * (b / (1 - x))\n",
        "\n",
        "# TT feature extraction\n",
        "def get_chaos_feature(data, q, epsilon, b, max_iter):\n",
        "    features = np.zeros_like(data, dtype=float)\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            w = q\n",
        "            n = 0\n",
        "            target = data[i, j]\n",
        "            while abs(target - w) > epsilon and n < max_iter:\n",
        "                w = gls_map(w, b)\n",
        "                n += 1\n",
        "            features[i, j] = (n / max_iter) * (1 - abs(target - w))\n",
        "    return features\n",
        "\n",
        "# Load and preprocess Iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))  # Min-max to [0, 1]\n",
        "y = iris.target\n",
        "\n",
        "# 5 samples per class\n",
        "np.random.seed(42)\n",
        "train_idx = []\n",
        "for c in range(3):\n",
        "    idx = np.random.choice(np.where(y == c)[0], 5, replace=False)\n",
        "    train_idx.extend(idx)\n",
        "test_idx = [i for i in range(len(y)) if i not in train_idx]\n",
        "\n",
        "X_train = X[train_idx]\n",
        "y_train = y[train_idx]\n",
        "X_test = X[test_idx]\n",
        "y_test = y[test_idx]\n",
        "\n",
        "# ChaosNet params\n",
        "q = 0.1           # ChaosNet default\n",
        "b = 0.4           # Bump b\n",
        "epsilon = 0.001   # ChaosNet tight epsilon\n",
        "max_iter = 5000   # Stable\n",
        "\n",
        "# Get TT features\n",
        "train_features = get_chaos_feature(X_train, q, epsilon, b, max_iter)\n",
        "print(\"Training TT features:\\n\", train_features)\n",
        "class_means = [train_features[y_train == c].mean(axis=0) for c in range(3)]\n",
        "print(\"Mean TT vectors:\\n\", class_means)\n",
        "test_features = get_chaos_feature(X_test, q, epsilon, b, max_iter)\n",
        "\n",
        "# ChaosNet mean vector Euclidean distance classification\n",
        "predictions = []\n",
        "for i in range(test_features.shape[0]):\n",
        "    f = test_features[i]\n",
        "    distances = [np.linalg.norm(f - m) for m in class_means]\n",
        "    pred = np.argmin(distances)  # Closest mean vector\n",
        "    predictions.append(pred)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = np.mean(predictions == y_test) * 100\n",
        "print(f\"GLS+TT Accuracy (5 samples/class): {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finding Datasets:"
      ],
      "metadata": {
        "id": "xWLAanO5DFbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta=ANMO&loc=00&cha=BHZ&start=2020-01-01T00:00:00&end=2020-01-01T01:00:00&format=miniseed\" -O quake_data.mseed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XQMmwfgSLFWn",
        "outputId": "e7779511-eb39-4a4c-885a-9589e9a21345"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-27 08:29:12--  http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta=ANMO&loc=00&cha=BHZ&start=2020-01-01T00:00:00&end=2020-01-01T01:00:00&format=miniseed\n",
            "Resolving service.iris.edu (service.iris.edu)... 128.95.166.47\n",
            "Connecting to service.iris.edu (service.iris.edu)|128.95.166.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: unspecified [application/vnd.fdsn.mseed]\n",
            "Saving to: ‘quake_data.mseed’\n",
            "\n",
            "quake_data.mseed        [ <=>                ] 131.00K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-03-27 08:29:13 (2.51 MB/s) - ‘quake_data.mseed’ saved [134144]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from obspy import read\n",
        "st = read(\"quake_data.mseed\")\n",
        "print(st)  # Waveform info\n",
        "trace = st[0].data  # NumPy array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am5x7X7XuKRc",
        "outputId": "f233c903-ff52-4135-da44-9fa3587f89c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Trace(s) in Stream:\n",
            "IU.ANMO.00.BHZ | 2020-01-01T00:00:00.019538Z - 2020-01-01T00:59:59.994538Z | 40.0 Hz, 144000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Collection:"
      ],
      "metadata": {
        "id": "1FWQIXVNDC04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from obspy import read\n",
        "import os\n",
        "\n",
        "stations = [\"ANMO\", \"COLA\", \"HRV\", \"KBS\", \"MAJO\", \"TATO\", \"POHA\"]\n",
        "base_quake = \"http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta={}&loc=00&cha=BHZ&start=2020-{:02d}-01T00:00:00&end=2020-{:02d}-01T00:01:00\"\n",
        "base_noise = \"http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&sta={}&loc=00&cha=BHZ&start=2020-{:02d}-02T00:00:00&end=2020-{:02d}-02T00:01:00\"\n",
        "\n",
        "print(\"Fetching until 75 quakes + 75 noise...\")\n",
        "X_quake, X_noise = [], []\n",
        "i = 0\n",
        "while len(X_quake) < 75 or len(X_noise) < 75:\n",
        "    month = (i // 7) + 1  # Cycle months 1-12\n",
        "    sta_idx = i % 7       # Cycle 7 stations\n",
        "    if month > 12:        # Wrap around years if needed\n",
        "        month = month % 12 or 12\n",
        "\n",
        "    # Quake\n",
        "    if len(X_quake) < 75:\n",
        "        quake_url = base_quake.format(stations[sta_idx], month, month)\n",
        "        !wget -q \"{quake_url}\" -O \"quake_{i}.mseed\"\n",
        "        if os.path.getsize(f\"quake_{i}.mseed\") > 0:\n",
        "            try:\n",
        "                st = read(f\"quake_{i}.mseed\")\n",
        "                trace = st[0].data\n",
        "                if len(trace) < 2400:\n",
        "                    trace = np.pad(trace, (0, 2400 - len(trace)), mode='constant')\n",
        "                X_quake.append(trace[:2400])\n",
        "                print(f\"Quake {len(X_quake)} fetched: {len(trace)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"Quake {i} failed: {e}\")\n",
        "        else:\n",
        "            print(f\"Quake {i} empty—skipping\")\n",
        "\n",
        "    # Noise\n",
        "    if len(X_noise) < 75:\n",
        "        noise_url = base_noise.format(stations[sta_idx], month, month)\n",
        "        !wget -q \"{noise_url}\" -O \"noise_{i}.mseed\"\n",
        "        if os.path.getsize(f\"noise_{i}.mseed\") > 0:\n",
        "            try:\n",
        "                st = read(f\"noise_{i}.mseed\")\n",
        "                trace = st[0].data\n",
        "                if len(trace) < 2400:\n",
        "                    trace = np.pad(trace, (0, 2400 - len(trace)), mode='constant')\n",
        "                X_noise.append(trace[:2400])\n",
        "                print(f\"Noise {len(X_noise)} fetched: {len(trace)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"Noise {i} failed: {e}\")\n",
        "        else:\n",
        "            print(f\"Noise {i} empty—skipping\")\n",
        "\n",
        "    i += 1\n",
        "    if i > 200:  # Safety cap—adjust if needed\n",
        "        print(\"Hit 200 attempts—stopping early\")\n",
        "        break\n",
        "\n",
        "print(f\"Quakes collected: {len(X_quake)}, Noise collected: {len(X_noise)}\")\n",
        "X = np.array(X_quake + X_noise)\n",
        "y = np.array([1] * len(X_quake) + [0] * len(X_noise))\n",
        "X = (X - X.min(axis=1, keepdims=True)) / (X.max(axis=1, keepdims=True) - X.min(axis=1, keepdims=True))\n",
        "\n",
        "subset_file = \"iris_subset_150.npz\"\n",
        "np.savez(subset_file, X=X, y=y)\n",
        "print(f\"Subset saved to: {subset_file}\")\n",
        "print(\"Subset shape:\", X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV868kLIvC_r",
        "outputId": "e903c4ce-3914-47d1-e403-bef5dc960921"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching until 75 quakes + 75 noise...\n",
            "Quake 1 fetched: 2400 samples\n",
            "Noise 1 fetched: 2400 samples\n",
            "Quake 2 fetched: 2400 samples\n",
            "Noise 2 fetched: 2400 samples\n",
            "Quake 3 fetched: 2400 samples\n",
            "Noise 3 fetched: 2400 samples\n",
            "Quake 4 fetched: 2400 samples\n",
            "Noise 4 fetched: 2400 samples\n",
            "Quake 5 fetched: 2400 samples\n",
            "Noise 5 fetched: 2400 samples\n",
            "Quake 6 fetched: 2400 samples\n",
            "Noise 6 fetched: 2400 samples\n",
            "Quake 7 fetched: 2400 samples\n",
            "Noise 7 fetched: 2400 samples\n",
            "Quake 8 fetched: 2400 samples\n",
            "Noise 8 fetched: 2400 samples\n",
            "Quake 9 fetched: 2400 samples\n",
            "Noise 9 fetched: 2400 samples\n",
            "Quake 10 fetched: 2400 samples\n",
            "Noise 10 fetched: 2400 samples\n",
            "Quake 11 fetched: 2400 samples\n",
            "Noise 11 fetched: 2400 samples\n",
            "Quake 12 fetched: 2400 samples\n",
            "Noise 12 fetched: 2400 samples\n",
            "Quake 13 fetched: 2400 samples\n",
            "Noise 13 fetched: 2400 samples\n",
            "Quake 14 fetched: 2400 samples\n",
            "Noise 14 fetched: 2400 samples\n",
            "Quake 15 fetched: 2400 samples\n",
            "Noise 15 fetched: 2400 samples\n",
            "Quake 16 fetched: 2400 samples\n",
            "Noise 16 fetched: 2400 samples\n",
            "Quake 17 fetched: 2400 samples\n",
            "Noise 17 fetched: 2400 samples\n",
            "Quake 18 fetched: 2400 samples\n",
            "Noise 18 fetched: 2400 samples\n",
            "Quake 19 fetched: 2400 samples\n",
            "Noise 19 fetched: 2400 samples\n",
            "Quake 20 fetched: 2400 samples\n",
            "Noise 20 fetched: 2400 samples\n",
            "Quake 21 fetched: 2400 samples\n",
            "Noise 21 fetched: 2400 samples\n",
            "Quake 22 fetched: 2400 samples\n",
            "Noise 22 fetched: 2400 samples\n",
            "Quake 23 fetched: 2400 samples\n",
            "Noise 23 fetched: 2400 samples\n",
            "Quake 24 fetched: 2400 samples\n",
            "Noise 24 fetched: 2400 samples\n",
            "Quake 25 fetched: 2400 samples\n",
            "Noise 25 fetched: 2400 samples\n",
            "Quake 26 fetched: 2400 samples\n",
            "Noise 26 fetched: 2400 samples\n",
            "Quake 27 fetched: 2400 samples\n",
            "Noise 27 fetched: 2400 samples\n",
            "Quake 28 fetched: 2400 samples\n",
            "Noise 28 fetched: 2400 samples\n",
            "Quake 29 fetched: 2400 samples\n",
            "Noise 29 fetched: 2400 samples\n",
            "Quake 30 fetched: 2400 samples\n",
            "Noise 30 fetched: 2400 samples\n",
            "Quake 31 fetched: 2400 samples\n",
            "Noise 31 fetched: 2400 samples\n",
            "Quake 32 fetched: 2400 samples\n",
            "Noise 32 fetched: 2400 samples\n",
            "Quake 33 fetched: 2400 samples\n",
            "Noise 33 fetched: 2400 samples\n",
            "Quake 34 fetched: 2400 samples\n",
            "Noise 34 fetched: 2400 samples\n",
            "Quake 35 fetched: 2400 samples\n",
            "Noise 35 fetched: 2400 samples\n",
            "Quake 36 fetched: 2400 samples\n",
            "Noise 36 fetched: 2400 samples\n",
            "Quake 37 fetched: 2400 samples\n",
            "Noise 37 fetched: 2400 samples\n",
            "Quake 38 fetched: 2400 samples\n",
            "Noise 38 fetched: 2400 samples\n",
            "Quake 39 fetched: 2400 samples\n",
            "Noise 39 fetched: 2400 samples\n",
            "Quake 40 fetched: 2400 samples\n",
            "Noise 40 fetched: 2400 samples\n",
            "Quake 41 fetched: 2400 samples\n",
            "Noise 41 fetched: 2400 samples\n",
            "Quake 42 fetched: 2400 samples\n",
            "Noise 42 fetched: 2400 samples\n",
            "Quake 43 fetched: 2400 samples\n",
            "Noise 43 fetched: 2400 samples\n",
            "Quake 44 fetched: 2400 samples\n",
            "Noise 44 fetched: 2400 samples\n",
            "Quake 45 fetched: 2400 samples\n",
            "Noise 45 fetched: 2400 samples\n",
            "Quake 46 fetched: 2400 samples\n",
            "Noise 46 fetched: 2400 samples\n",
            "Quake 47 fetched: 2400 samples\n",
            "Noise 47 fetched: 2400 samples\n",
            "Quake 48 fetched: 2400 samples\n",
            "Noise 48 fetched: 2400 samples\n",
            "Quake 49 fetched: 2400 samples\n",
            "Noise 49 fetched: 2400 samples\n",
            "Quake 50 fetched: 2400 samples\n",
            "Noise 50 fetched: 2400 samples\n",
            "Quake 51 fetched: 2400 samples\n",
            "Noise 51 fetched: 2400 samples\n",
            "Quake 52 fetched: 2400 samples\n",
            "Noise 52 fetched: 2400 samples\n",
            "Quake 53 fetched: 2400 samples\n",
            "Noise 53 fetched: 2400 samples\n",
            "Quake 54 fetched: 2400 samples\n",
            "Noise 54 fetched: 2400 samples\n",
            "Quake 55 fetched: 2400 samples\n",
            "Noise 55 fetched: 2400 samples\n",
            "Quake 56 fetched: 2400 samples\n",
            "Noise 56 fetched: 2400 samples\n",
            "Quake 57 fetched: 2400 samples\n",
            "Noise 57 fetched: 2400 samples\n",
            "Quake 58 fetched: 2400 samples\n",
            "Noise 58 fetched: 2400 samples\n",
            "Quake 59 fetched: 2400 samples\n",
            "Noise 59 fetched: 2400 samples\n",
            "Quake 60 fetched: 2400 samples\n",
            "Noise 60 fetched: 2400 samples\n",
            "Quake 61 fetched: 2400 samples\n",
            "Noise 61 fetched: 2400 samples\n",
            "Quake 62 fetched: 2400 samples\n",
            "Noise 62 fetched: 2400 samples\n",
            "Quake 63 fetched: 2400 samples\n",
            "Noise 63 fetched: 2400 samples\n",
            "Quake 64 fetched: 2400 samples\n",
            "Noise 64 fetched: 2400 samples\n",
            "Quake 65 fetched: 2400 samples\n",
            "Noise 65 fetched: 2400 samples\n",
            "Quake 66 fetched: 2400 samples\n",
            "Noise 66 fetched: 2400 samples\n",
            "Quake 66 empty—skipping\n",
            "Noise 66 empty—skipping\n",
            "Quake 67 fetched: 2400 samples\n",
            "Noise 67 fetched: 2400 samples\n",
            "Quake 68 fetched: 2400 samples\n",
            "Noise 68 fetched: 2400 samples\n",
            "Quake 69 fetched: 2400 samples\n",
            "Noise 69 fetched: 2400 samples\n",
            "Quake 70 fetched: 2400 samples\n",
            "Noise 70 fetched: 2400 samples\n",
            "Quake 71 fetched: 2400 samples\n",
            "Noise 71 fetched: 2400 samples\n",
            "Quake 72 fetched: 2400 samples\n",
            "Noise 72 fetched: 2400 samples\n",
            "Quake 73 empty—skipping\n",
            "Noise 73 empty—skipping\n",
            "Quake 73 fetched: 2400 samples\n",
            "Noise 73 fetched: 2400 samples\n",
            "Quake 74 fetched: 2400 samples\n",
            "Noise 74 fetched: 2400 samples\n",
            "Quake 75 fetched: 2400 samples\n",
            "Noise 75 fetched: 2400 samples\n",
            "Quakes collected: 75, Noise collected: 75\n",
            "Subset saved to: iris_subset_150.npz\n",
            "Subset shape: (150, 2400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##HYPER PARAMETER TUNING:"
      ],
      "metadata": {
        "id": "nHPOmlD2C4pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "subset_file = \"iris_subset_150.npz\"\n",
        "loaded = np.load(subset_file)\n",
        "X = loaded[\"X\"][70:80]  # 5 quakes, 5 noise from your working slice\n",
        "y = loaded[\"y\"][70:80]\n",
        "print(\"y full:\", y)  # [1 1 1 1 1 0 0 0 0 0]\n",
        "\n",
        "\n",
        "X_train = np.concatenate([X[:3], X[5:8]])  # 3 quakes, 3 noise\n",
        "y_train = np.concatenate([y[:3], y[5:8]])\n",
        "X_test = np.concatenate([X[3:5], X[8:]])   # 2 quakes, 2 noise\n",
        "y_test = np.concatenate([y[3:5], y[8:]])\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n",
        "\n",
        "def gls_map(x, b, scale=0.7):\n",
        "    if 0 <= x < b:\n",
        "        return x\n",
        "    return scale * (b / (1 - x))\n",
        "\n",
        "def get_chaos_feature(data, q, epsilon, b, max_iter):\n",
        "    features = np.zeros_like(data, dtype=float)\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            w = q\n",
        "            n = 0\n",
        "            target = data[i, j]\n",
        "            while abs(target - w) > epsilon and n < max_iter:\n",
        "                w = gls_map(w, b)\n",
        "                n += 1\n",
        "            features[i, j] = (n / max_iter) * (1 - abs(target - w))\n",
        "    return features\n",
        "\n",
        "params = [\n",
        "    {\"q\": 0.7, \"b\": 0.5, \"epsilon\": 0.1},   # 100% winner\n",
        "    {\"q\": 0.1, \"b\": 0.5, \"epsilon\": 0.2},   # 75%\n",
        "    {\"q\": 0.4, \"b\": 0.2, \"epsilon\": 0.005}, # 50%\n",
        "    {\"q\": 0.5, \"b\": 0.4, \"epsilon\": 0.05},  # New combo\n",
        "    {\"q\": 0.3, \"b\": 0.6, \"epsilon\": 0.01}   # New combo\n",
        "]\n",
        "max_iter = 5000\n",
        "\n",
        "for p in params:\n",
        "    q, b, epsilon = p[\"q\"], p[\"b\"], p[\"epsilon\"]\n",
        "    train_features = get_chaos_feature(X_train, q, epsilon, b, max_iter)\n",
        "    class_means = [train_features[y_train == c].mean(axis=0) for c in [0, 1]]\n",
        "    if np.any(np.isnan(class_means)):\n",
        "        print(f\"NaN in class_means for q={q}, b={b}, epsilon={epsilon}\")\n",
        "        continue\n",
        "    test_features = get_chaos_feature(X_test, q, epsilon, b, max_iter)\n",
        "    predictions = [np.argmin([np.linalg.norm(test_features[i] - m) for m in class_means]) for i in range(test_features.shape[0])]\n",
        "    accuracy = np.mean(predictions == y_test) * 100\n",
        "    print(f\"q={q}, b={b}, epsilon={epsilon}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TWWBqLjVvLne",
        "outputId": "238de4ab-4ab9-4bf6-b20a-110d3ea9134d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y full: [1 1 1 1 1 0 0 0 0 0]\n",
            "Train shape: (6, 2400)\n",
            "Test shape: (4, 2400)\n",
            "y_train: [1 1 1 0 0 0]\n",
            "y_test: [1 1 0 0]\n",
            "q=0.7, b=0.5, epsilon=0.1: 100.00%\n",
            "q=0.1, b=0.5, epsilon=0.2: 75.00%\n",
            "q=0.4, b=0.2, epsilon=0.005: 50.00%\n",
            "q=0.5, b=0.4, epsilon=0.05: 75.00%\n",
            "q=0.3, b=0.6, epsilon=0.01: 75.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "subset_file = \"iris_subset_150.npz\"\n",
        "loaded = np.load(subset_file)\n",
        "X = loaded[\"X\"][65:85]  # 5 quakes, 5 noise from your working slice\n",
        "y = loaded[\"y\"][65:85]\n",
        "print(\"y full:\", y)  # [1 1 1 1 1 0 0 0 0 0]\n",
        "\n",
        "\n",
        "X_train = np.concatenate([X[:5], X[10:15]])  # 3 quakes, 3 noise\n",
        "y_train = np.concatenate([y[:5], y[10:15]])\n",
        "X_test = np.concatenate([X[5:10], X[15:]])   # 2 quakes, 2 noise\n",
        "y_test = np.concatenate([y[5:10], y[15:]])\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "print(\"y_train:\", y_train)\n",
        "print(\"y_test:\", y_test)\n",
        "\n",
        "def gls_map(x, b, scale=0.7):\n",
        "    if 0 <= x < b:\n",
        "        return x\n",
        "    return scale * (b / (1 - x))\n",
        "\n",
        "def get_chaos_feature(data, q, epsilon, b, max_iter):\n",
        "    features = np.zeros_like(data, dtype=float)\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            w = q\n",
        "            n = 0\n",
        "            target = data[i, j]\n",
        "            while abs(target - w) > epsilon and n < max_iter:\n",
        "                w = gls_map(w, b)\n",
        "                n += 1\n",
        "            features[i, j] = (n / max_iter) * (1 - abs(target - w))\n",
        "    return features\n",
        "\n",
        "params = [\n",
        "    {\"q\": 0.7, \"b\": 0.5, \"epsilon\": 0.1},   # 100% winner\n",
        "    {\"q\": 0.1, \"b\": 0.5, \"epsilon\": 0.2},   # 75%\n",
        "    {\"q\": 0.4, \"b\": 0.2, \"epsilon\": 0.005}, # 50%\n",
        "    {\"q\": 0.5, \"b\": 0.4, \"epsilon\": 0.05},  # New combo\n",
        "    {\"q\": 0.3, \"b\": 0.6, \"epsilon\": 0.01}   # New combo\n",
        "]\n",
        "max_iter = 5000\n",
        "\n",
        "for p in params:\n",
        "    q, b, epsilon = p[\"q\"], p[\"b\"], p[\"epsilon\"]\n",
        "    train_features = get_chaos_feature(X_train, q, epsilon, b, max_iter)\n",
        "    class_means = [train_features[y_train == c].mean(axis=0) for c in [0, 1]]\n",
        "    if np.any(np.isnan(class_means)):\n",
        "        print(f\"NaN in class_means for q={q}, b={b}, epsilon={epsilon}\")\n",
        "        continue\n",
        "    test_features = get_chaos_feature(X_test, q, epsilon, b, max_iter)\n",
        "    predictions = [np.argmin([np.linalg.norm(test_features[i] - m) for m in class_means]) for i in range(test_features.shape[0])]\n",
        "    accuracy = np.mean(predictions == y_test) * 100\n",
        "    print(f\"q={q}, b={b}, epsilon={epsilon}: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8niA5a1W6jLR",
        "outputId": "021f09b4-26d3-4ff4-bddf-523b084ab549"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y full: [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
            "Train shape: (10, 2400)\n",
            "Test shape: (10, 2400)\n",
            "y_train: [1 1 1 1 1 0 0 0 0 0]\n",
            "y_test: [1 1 1 1 1 0 0 0 0 0]\n",
            "q=0.7, b=0.5, epsilon=0.1: 70.00%\n",
            "q=0.1, b=0.5, epsilon=0.2: 50.00%\n",
            "q=0.4, b=0.2, epsilon=0.005: 40.00%\n",
            "q=0.5, b=0.4, epsilon=0.05: 80.00%\n",
            "q=0.3, b=0.6, epsilon=0.01: 60.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "\n",
        "- **What We Did:**\n",
        "\n",
        " - Used ChaosNet (from the paper) to classify IRIS seismic data (quake vs. noise).\n",
        "\n",
        " - Started with Iris dataset - 94.81% accuracy with 5 samples/class.\n",
        "\n",
        "- **Seismic Results:**\n",
        " - Fetched 75 quakes + 75 noise (2400 samples each).\n",
        " - First try - 50% on 5/140 split, random (50%).\n",
        "- **Tuning:**\n",
        " - On a small chunk `[70:80]`, hit 100% (`q=0.7, b=0.5, epsilon=0.1`) and 75% (`q=0.5, b=0.4, epsilon=0.05`).\n",
        "  - Bigger slice `[65:85]` got 80%.\n",
        "- **What’s Up:**\n",
        " - Seismic’s 2400D is trickier than Iris’s 4D. Tuning `q`, `b`, `epsilon`.\n",
        "\n",
        "- **Anomaly Detection Plan:**\n",
        " - Wanted to spot quakes in 20-min streams.\n",
        " - Got 1-min chunks working, but long quake data’s rare - IRIS fetches kept failing (empty files).\n",
        "- **Where We’re At:**\n",
        " - Classification’s working (80%), but anomaly detection’s early - needs more data and ChaosNet tweaks.\n",
        "- **Chaos Theory Scope:** Perfect for seismic work - TT’s just step one. Could work on anomaly hunting.\n",
        "- **Next:**\n",
        " - Test 5/140 with top params, push past 80%.\n",
        " - Take it a step further, work on anomaly detection in earthquakes (Initial data acquired - 5 noise, 5 quakes - of varying length)."
      ],
      "metadata": {
        "id": "1UFdPIvuUzD5"
      }
    }
  ]
}